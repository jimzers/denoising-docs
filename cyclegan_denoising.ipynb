{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA stuff\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use cycleGAN\n",
    "\n",
    "## what to use?\n",
    "\n",
    "for generator, use encoder/transformer/decoder combo\n",
    "\n",
    "for discriminator, use patchGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making generator\n",
    "\n",
    "we gotta make residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def activation_func(activation):\n",
    "#     return  nn.ModuleDict([\n",
    "#         ['relu', nn.ReLU(inplace=True)],\n",
    "#         ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "#         ['selu', nn.SELU(inplace=True)],\n",
    "#         ['none', nn.Identity()]\n",
    "#     ])[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation_fn):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.shortcut = nn.Identity()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.apply_shortcut:\n",
    "            residual = self.shortcut(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, activation_fn):\n",
    "        super(ResNetResidualBlock, self).__init__(in_channels, \n",
    "                                                       out_channels, \n",
    "                                                       *args,\n",
    "                                                       **kwargs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        # input and output dim will be the same for our uses\n",
    "        self.conv1 = nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1, bias=True)\n",
    "        self.norm1 = nn.InstanceNorm2d(input_dim)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1, bias=True)\n",
    "        self.norm2 = nn.InstanceNorm2d(input_dim)\n",
    "        \n",
    "#         self.relu_final = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_new = self.conv1(x)\n",
    "        x_new = self.norm1(x_new)\n",
    "        x_new = self.relu1(x_new)\n",
    "        x_new = self.conv2(x_new)\n",
    "        x_new = self.norm2(x_new)\n",
    "        out = x + x_new\n",
    "#         out = self.relu_final(x_new)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CycleGenerator, self).__init__()\n",
    "#         self.activations = nn.ModuleDict({\n",
    "#         'relu', nn.ReLU(inplace=True),\n",
    "#         'leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "#         'selu', nn.SELU(inplace=True),\n",
    "#         'none', nn.Identity()})\n",
    "        \n",
    "        # do we need this many filter channels \n",
    "        # if we're doing a 1 channel image rather than 3 channel?\n",
    "        \n",
    "#         #encoder section\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=64,\n",
    "#                                kernel_size=(7, 7), padding=0)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1), stride=2)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1), stride=2)\n",
    "        \n",
    "#         # in the transformer\n",
    "#         self.conv4 = nn.Conv2d(1, 128, (3, 3), padding=(1, 1), stride=2)\n",
    "#         https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(in_channels=1, out_channels=64,\n",
    "                           kernel_size=7, padding=0,\n",
    "                           bias=True),\n",
    "                 nn.InstanceNorm2d(64),\n",
    "                 nn.LeakyReLU(negative_slope=0.01, inplace=True)]\n",
    "    \n",
    "        #downsampling layers\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(in_channels=64*mult, out_channels=64*mult*2,\n",
    "                           kernel_size=3, stride=2, padding=1,\n",
    "                           bias=True),\n",
    "                      nn.InstanceNorm2d(64*mult*2),\n",
    "                      nn.LeakyReLU(negative_slope=0.01, inplace=True)]\n",
    "            \n",
    "        # resnet blocks layer\n",
    "        num_resnet_blocks = 6\n",
    "        for i in range(num_resnet_blocks):\n",
    "            model += [ResNetBlock(64*mult*2)]\n",
    "            \n",
    "        # upsampling layers\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(in_channels=64*mult, out_channels=int(64*mult/2),\n",
    "                           kernel_size=3, stride=2, padding=1,\n",
    "                           bias=True),\n",
    "                      nn.InstanceNorm2d(int(64*mult/2)),\n",
    "                      nn.LeakyReLU(negative_slope=0.01, inplace=True)]\n",
    "            \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CycleDiscriminator, self).__init__()\n",
    "        #https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/models.py\n",
    "        \n",
    "        model = [nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2, True)]\n",
    "        n_layers = 3\n",
    "        \n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8) # 2^n\n",
    "            model += [\n",
    "                nn.Conv2d(64 * nf_mult_prev, 64 * nf_mult, stride=2, padding=1, bias=True),\n",
    "                nn.InstanceNorm2d(64*nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n, 8) # 2^n\n",
    "        model += [\n",
    "            nn.Conv2d(64 * nf_mult_prev, 64 * nf_mult, stride=1, padding=1, bias=True),\n",
    "            nn.InstanceNorm2d(64*nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        model += [nn.Conv2d(64*nf_mult, 1, kernel_size=4, stride=1, padding=1)] # 1 channel prediction map\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/utils.py\n",
    "class ImageBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "    \n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for elem in data.data:\n",
    "            elem = torch.unsqueeze(elem, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(elem)\n",
    "                to_return.append(elem)\n",
    "            else:\n",
    "                # half chance to randomly pick a data pt from history, \n",
    "                # otherwise just pick the selected data pt and don't add to history \n",
    "                if random.uniform(0, 1) < 0.5:\n",
    "                    idx = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = elem\n",
    "                else:\n",
    "                    to_return.append(elem)\n",
    "        return torch.cat(to_return)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentsDataset(Dataset):\n",
    "    def __init__(self, dirty_dir, clean_dir, transform=None, aligned=False):\n",
    "        super(DocumentsDataset, self).__init__()\n",
    "        self.dirty_arr = glob.glob(os.path.join(dirty_dir, '*.png'))\n",
    "        self.clean_arr = glob.glob(os.path.join(clean_dir, '*.png'))\n",
    "        self.dirty_dir = dirty_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        self.aligned = aligned\n",
    "    \n",
    "    def __len__(self):\n",
    "        # could be dirty arr or clean arr, shouldn't matter\n",
    "        return max(len(self.dirty_arr), len(self.clean_arr))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one sample of data, based on dirty data\"\"\"\n",
    "        \n",
    "        # grab a random index for each. the modulus allows overflow, and makes it unaligned if overflow\n",
    "        idx_dirty = index % len(self.dirty_arr)\n",
    "        if self.aligned:\n",
    "            idx_clean = index % len(self.clean_arr)\n",
    "        else:\n",
    "            idx_clean = random.randint(0, len(self.clean_arr) - 1)\n",
    "        \n",
    "        dirty_img_name = os.path.basename(self.dirty_arr[idx_dirty])\n",
    "        clean_img_name = os.path.basename(self.clean_arr[idx_clean])\n",
    "        dirty_path = os.path.join(self.dirty_dir, dirty_img_name)\n",
    "        clean_path = os.path.join(self.clean_dir, clean_img_name)\n",
    "        transformed_dirty = Image.open(dirty_path)\n",
    "        transformed_clean = Image.open(clean_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            # notice how with each transform, they are each independent.\n",
    "            # this allows the random crop and flips to be different with each img\n",
    "            # because cyclegan is meant for unpaired it won't matter\n",
    "            transformed_dirty = self.transform(transformed_dirty)\n",
    "            transformed_clean = self.transform(transformed_clean)\n",
    "            \n",
    "        # because the image is single channel, we need to unsqueeze so it shows that single channel.\n",
    "        transformed_dirty = torch.unsqueeze(transformed_dirty, dim=0)\n",
    "        transformed_clean = torch.unsqueeze(transformed_clean, dim=0)\n",
    "        \n",
    "        return {'dirty': transformed_dirty, 'clean': transformed_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = Image.open('./data/train/101.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 540)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(arr, dtype=np.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train\\\\101.png',\n",
       " './data/train\\\\102.png',\n",
       " './data/train\\\\104.png',\n",
       " './data/train\\\\105.png',\n",
       " './data/train\\\\107.png',\n",
       " './data/train\\\\108.png',\n",
       " './data/train\\\\11.png',\n",
       " './data/train\\\\110.png',\n",
       " './data/train\\\\111.png',\n",
       " './data/train\\\\113.png',\n",
       " './data/train\\\\114.png',\n",
       " './data/train\\\\116.png',\n",
       " './data/train\\\\117.png',\n",
       " './data/train\\\\119.png',\n",
       " './data/train\\\\12.png',\n",
       " './data/train\\\\120.png',\n",
       " './data/train\\\\122.png',\n",
       " './data/train\\\\123.png',\n",
       " './data/train\\\\125.png',\n",
       " './data/train\\\\126.png',\n",
       " './data/train\\\\128.png',\n",
       " './data/train\\\\129.png',\n",
       " './data/train\\\\131.png',\n",
       " './data/train\\\\132.png',\n",
       " './data/train\\\\134.png',\n",
       " './data/train\\\\135.png',\n",
       " './data/train\\\\137.png',\n",
       " './data/train\\\\138.png',\n",
       " './data/train\\\\14.png',\n",
       " './data/train\\\\140.png',\n",
       " './data/train\\\\141.png',\n",
       " './data/train\\\\143.png',\n",
       " './data/train\\\\144.png',\n",
       " './data/train\\\\146.png',\n",
       " './data/train\\\\147.png',\n",
       " './data/train\\\\149.png',\n",
       " './data/train\\\\15.png',\n",
       " './data/train\\\\150.png',\n",
       " './data/train\\\\152.png',\n",
       " './data/train\\\\153.png',\n",
       " './data/train\\\\155.png',\n",
       " './data/train\\\\156.png',\n",
       " './data/train\\\\158.png',\n",
       " './data/train\\\\159.png',\n",
       " './data/train\\\\161.png',\n",
       " './data/train\\\\162.png',\n",
       " './data/train\\\\164.png',\n",
       " './data/train\\\\165.png',\n",
       " './data/train\\\\167.png',\n",
       " './data/train\\\\168.png',\n",
       " './data/train\\\\17.png',\n",
       " './data/train\\\\170.png',\n",
       " './data/train\\\\171.png',\n",
       " './data/train\\\\173.png',\n",
       " './data/train\\\\174.png',\n",
       " './data/train\\\\176.png',\n",
       " './data/train\\\\177.png',\n",
       " './data/train\\\\179.png',\n",
       " './data/train\\\\18.png',\n",
       " './data/train\\\\180.png',\n",
       " './data/train\\\\182.png',\n",
       " './data/train\\\\183.png',\n",
       " './data/train\\\\185.png',\n",
       " './data/train\\\\186.png',\n",
       " './data/train\\\\188.png',\n",
       " './data/train\\\\189.png',\n",
       " './data/train\\\\191.png',\n",
       " './data/train\\\\192.png',\n",
       " './data/train\\\\194.png',\n",
       " './data/train\\\\195.png',\n",
       " './data/train\\\\197.png',\n",
       " './data/train\\\\198.png',\n",
       " './data/train\\\\2.png',\n",
       " './data/train\\\\20.png',\n",
       " './data/train\\\\200.png',\n",
       " './data/train\\\\201.png',\n",
       " './data/train\\\\203.png',\n",
       " './data/train\\\\204.png',\n",
       " './data/train\\\\206.png',\n",
       " './data/train\\\\207.png',\n",
       " './data/train\\\\209.png',\n",
       " './data/train\\\\21.png',\n",
       " './data/train\\\\210.png',\n",
       " './data/train\\\\212.png',\n",
       " './data/train\\\\213.png',\n",
       " './data/train\\\\215.png',\n",
       " './data/train\\\\216.png',\n",
       " './data/train\\\\23.png',\n",
       " './data/train\\\\24.png',\n",
       " './data/train\\\\26.png',\n",
       " './data/train\\\\27.png',\n",
       " './data/train\\\\29.png',\n",
       " './data/train\\\\3.png',\n",
       " './data/train\\\\30.png',\n",
       " './data/train\\\\32.png',\n",
       " './data/train\\\\33.png',\n",
       " './data/train\\\\35.png',\n",
       " './data/train\\\\36.png',\n",
       " './data/train\\\\38.png',\n",
       " './data/train\\\\39.png',\n",
       " './data/train\\\\41.png',\n",
       " './data/train\\\\42.png',\n",
       " './data/train\\\\44.png',\n",
       " './data/train\\\\45.png',\n",
       " './data/train\\\\47.png',\n",
       " './data/train\\\\48.png',\n",
       " './data/train\\\\5.png',\n",
       " './data/train\\\\50.png',\n",
       " './data/train\\\\51.png',\n",
       " './data/train\\\\53.png',\n",
       " './data/train\\\\54.png',\n",
       " './data/train\\\\56.png',\n",
       " './data/train\\\\57.png',\n",
       " './data/train\\\\59.png',\n",
       " './data/train\\\\6.png',\n",
       " './data/train\\\\60.png',\n",
       " './data/train\\\\62.png',\n",
       " './data/train\\\\63.png',\n",
       " './data/train\\\\65.png',\n",
       " './data/train\\\\66.png',\n",
       " './data/train\\\\68.png',\n",
       " './data/train\\\\69.png',\n",
       " './data/train\\\\71.png',\n",
       " './data/train\\\\72.png',\n",
       " './data/train\\\\74.png',\n",
       " './data/train\\\\75.png',\n",
       " './data/train\\\\77.png',\n",
       " './data/train\\\\78.png',\n",
       " './data/train\\\\8.png',\n",
       " './data/train\\\\80.png',\n",
       " './data/train\\\\81.png',\n",
       " './data/train\\\\83.png',\n",
       " './data/train\\\\84.png',\n",
       " './data/train\\\\86.png',\n",
       " './data/train\\\\87.png',\n",
       " './data/train\\\\89.png',\n",
       " './data/train\\\\9.png',\n",
       " './data/train\\\\90.png',\n",
       " './data/train\\\\92.png',\n",
       " './data/train\\\\93.png',\n",
       " './data/train\\\\95.png',\n",
       " './data/train\\\\96.png',\n",
       " './data/train\\\\98.png',\n",
       " './data/train\\\\99.png']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr = glob.glob(os.path.join('./data/train', '*.png'))\n",
    "test_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\adaml\\\\Documents\\\\denoising-dirty-documents\\\\data\\\\train\\\\104.png'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(test_arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-ec0159db917c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m trans = transforms.Compose([\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# make it bigger so random crop is more random\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_h' is not defined"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize((int(img_h*1.12),int(img_w*1.12)), Image.BICUBIC), # make it bigger so random crop is more random\n",
    "    transforms.RandomCrop((img_h, img_w)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5) # mean, std of each channel. can provice a tuple for 3 dim images\n",
    "])\n",
    "training_set = DocumentsDataset(dirty_dir='./data/train/', clean_dir='./data/train_cleaned/', transform=trans)\n",
    "training_generator = DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: unsqueeze the resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lr(n_epochs, start_epoch, decay_epoch):\n",
    "    '''\n",
    "    returns a function that calcs learning rate for given epoch.\n",
    "    n_epochs (int) -> number of planned epochs\n",
    "    start_epoch -> the epoch number that the optimizer starts on\n",
    "    decay_epoch -> the epoch number to start decaying at\n",
    "    '''\n",
    "    return lambda epoch: 1.0 - max(0, epoch + start_epoch - decay_epoch)/(n_epochs - decay_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "n_epochs = 0\n",
    "decay_epoch = 0\n",
    "lr = 0\n",
    "batch_size = 0\n",
    "# if resuming change this\n",
    "starting_epoch = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        init the cycle gan. add the loss functions\n",
    "        add the models.\n",
    "        add img buffer\n",
    "        '''\n",
    "        \n",
    "        #start with models\n",
    "        generator_clean = CycleGenerator() # dirty to clean\n",
    "        discriminator_clean = CycleDiscriminator() # clean is fake/real\n",
    "        \n",
    "        generator_dirty = CycleGenerator() # clean to dirty\n",
    "        discriminator_clean = CycleDiscriminator() # dirty is fake/real\n",
    "        \n",
    "        # turn on cuda\n",
    "        if use_cuda:\n",
    "            generator_clean.cuda()\n",
    "            discriminator_clean.cuda()\n",
    "            generator_dirty.cuda()\n",
    "            discriminator_dirty.cuda()\n",
    "        \n",
    "        # init weights for stuff, apparently kaiming is better for conv nets?\n",
    "        generator_clean.apply(nn.init.kaiming_uniform)\n",
    "        discriminator_clean.apply(nn.init.kaiming_uniform)\n",
    "        generator_dirty.apply(nn.init.kaiming_uniform)\n",
    "        discriminator_dirty.apply(nn.init.kaiming_uniform)\n",
    "        \n",
    "        # loss fns\n",
    "        # also can add identity loss mentioned, but it is mainly for color channels. it is l1 loss as well\n",
    "        loss_gan = nn.MSELoss()\n",
    "        loss_cycle = nn.L1Loss()\n",
    "        \n",
    "        # optimizers\n",
    "        # use default recommended values for first and second momentums on gradient accumulation\n",
    "        opt_generators = optim.Adam(itertools.chain(generator_clean.parameters(),\n",
    "                                                   generator_dirty.parameters()),\n",
    "                                   lr=lr, betas=(0.5, 0.999))\n",
    "        opt_d_clean = optim.Adam(discriminator_clean.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        opt_d_dirty = optim.Adam(discriminator_dirty.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        \n",
    "        scheduler_g = optim.lr_scheduler.LambdaLR(opt_generators, lr_lambda=custom_lr(n_epochs, starting_epoch, decay_epoch))\n",
    "        scheduler_d_clean = optim.lr_scheduler.LambdaLR(opt_d_clean, lr_lambda=custom_lr(n_epochs, starting_epoch, decay_epoch))\n",
    "        scheduler_d_dirty = optim.lr_scheduler.LambdaLR(opt_d_dirty, lr_lambda=custom_lr(n_epochs, starting_epoch, decay_epoch))\n",
    "        \n",
    "        Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
    "        \n",
    "        \n",
    "        \n",
    "        # image buffers for gans\n",
    "        fake_img_buffer_clean = ImageBuffer()\n",
    "        fake_img_buffer_dirty = ImageBuffer()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
